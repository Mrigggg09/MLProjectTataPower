{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeTccCvFzTN1",
        "outputId": "e45c3fed-b2a3-4258-fe68-8c44fa0389d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyzq8J_mzxMc",
        "outputId": "e013c503-0c7d-44d0-ecda-8e75580c5d0a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "path1=\"/content/drive/MyDrive/ML TATA power/MLTATAPOWER_TRAIN.csv\"\n",
        "path2=\"/content/drive/MyDrive/ML TATA power/ML EVAL.csv\"\n",
        "# Loading dataset.\n",
        "train_df = pd.read_csv(path1) # training data variable\n",
        "predict_df = pd.read_csv(path2) # testing data variable\n",
        "\n",
        "\n",
        "train_df.columns = train_df.columns.str.replace(' ', '_')\n",
        "predict_df.columns = predict_df.columns.str.replace(' ', '_')\n",
        "\n",
        "train = train_df.copy();\n",
        "test = predict_df.copy();"
      ],
      "metadata": {
        "id": "AUwOTUSlz4v5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Defining features and target variable for training\n",
        "X_train = train_df[['FLU_GAS_TEMP', 'STEAM_PRESSURE', 'TURBINE_SPEED', 'GENERATOR_VOLTAGE', 'AIR_QUALITY']]\n",
        "y_train = train_df['HEALTHINESS']\n",
        "\n",
        "# Standardizing features for training\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "# Splitting the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Building the neural network model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Defining features for prediction\n",
        "X_predict = predict_df[['FLU_GAS_TEMP', 'STEAM_PRESSURE', 'TURBINE_SPEED', 'GENERATOR_VOLTAGE', 'AIR_QUALITY']]\n",
        "\n",
        "# Standardizing features for prediction\n",
        "X_predict = scaler.transform(X_predict)\n",
        "\n",
        "# Making predictions on the prediction dataset\n",
        "predicted_probabilities = model.predict(X_predict)\n",
        "\n",
        "i=2\n",
        "\n",
        "# Printing the predicted probabilities for the prediction dataset\n",
        "print(\"\\nPredicted Healthiness percentage for \",i,\" index of the Evaluating dataset : \",predicted_probabilities[i]*100,\"%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSWNEPeM4K49",
        "outputId": "54082e0b-7d1c-42c5-f07a-8b96f6be9fb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "634/634 [==============================] - 3s 3ms/step - loss: 0.3503 - accuracy: 0.8310 - val_loss: 0.2382 - val_accuracy: 0.8939\n",
            "Epoch 2/10\n",
            "634/634 [==============================] - 2s 3ms/step - loss: 0.2254 - accuracy: 0.8999 - val_loss: 0.2039 - val_accuracy: 0.9132\n",
            "Epoch 3/10\n",
            "634/634 [==============================] - 2s 4ms/step - loss: 0.1921 - accuracy: 0.9178 - val_loss: 0.1761 - val_accuracy: 0.9237\n",
            "Epoch 4/10\n",
            "634/634 [==============================] - 3s 4ms/step - loss: 0.1662 - accuracy: 0.9299 - val_loss: 0.1572 - val_accuracy: 0.9347\n",
            "Epoch 5/10\n",
            "634/634 [==============================] - 2s 4ms/step - loss: 0.1447 - accuracy: 0.9411 - val_loss: 0.1378 - val_accuracy: 0.9446\n",
            "Epoch 6/10\n",
            "634/634 [==============================] - 3s 5ms/step - loss: 0.1270 - accuracy: 0.9496 - val_loss: 0.1238 - val_accuracy: 0.9521\n",
            "Epoch 7/10\n",
            "207/634 [========>.....................] - ETA: 1s - loss: 0.1224 - accuracy: 0.9517"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "QS85EllPeE8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Error description\n",
        "error_desc=['NO ERROR','ERROR A','ERROR B','ERROR C']\n",
        "\n",
        "# Defining features and target variable for training (for ERROR_TYPE)\n",
        "X_train_error = train_df[['FLU_GAS_TEMP', 'STEAM_PRESSURE', 'TURBINE_SPEED', 'GENERATOR_VOLTAGE', 'AIR_QUALITY']]\n",
        "y_train_error = train_df['ERROR_TYPE'].astype(int)  # Ensure labels are integers\n",
        "\n",
        "# Standardizing features for training (for ERROR_TYPE)\n",
        "scaler_error = StandardScaler()\n",
        "X_train_error = scaler_error.fit_transform(X_train_error)\n",
        "\n",
        "# Converting NumPy array back to DataFrame\n",
        "X_train_error = pd.DataFrame(X_train_error, columns=['FLU_GAS_TEMP', 'STEAM_PRESSURE', 'TURBINE_SPEED', 'GENERATOR_VOLTAGE', 'AIR_QUALITY'])\n",
        "\n",
        "# Splitting the dataset into training and validation sets (for ERROR_TYPE)\n",
        "X_train_error, X_val_error, y_train_error, y_val_error = train_test_split(\n",
        "    X_train_error, y_train_error, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Building the DNNClassifier for ERROR_TYPE\n",
        "feature_columns_error = [tf.feature_column.numeric_column(key=key) for key in X_train_error.columns]\n",
        "classifier_error = tf.estimator.DNNClassifier(\n",
        "    feature_columns=feature_columns_error,\n",
        "    hidden_units=[128, 64],\n",
        "    n_classes=4,  # Assuming 4 classes for ERROR_TYPE\n",
        ")\n",
        "\n",
        "# Input function for ERROR_TYPE\n",
        "input_fn_error = tf.compat.v1.estimator.inputs.pandas_input_fn(\n",
        "    x=X_train_error, y=y_train_error, batch_size=32, num_epochs=10, shuffle=True\n",
        ")\n",
        "\n",
        "# Training the DNNClassifier for ERROR_TYPE\n",
        "classifier_error.train(input_fn=input_fn_error)\n",
        "\n",
        "# Defining features for prediction (for ERROR_TYPE)\n",
        "X_predict_error = predict_df[['FLU_GAS_TEMP', 'STEAM_PRESSURE', 'TURBINE_SPEED', 'GENERATOR_VOLTAGE', 'AIR_QUALITY']]\n",
        "\n",
        "# Standardizing features for prediction (for ERROR_TYPE)\n",
        "X_predict_error = scaler_error.transform(X_predict_error)\n",
        "\n",
        "# Converting NumPy array back to DataFrame\n",
        "X_predict_error = pd.DataFrame(X_predict_error, columns=['FLU_GAS_TEMP', 'STEAM_PRESSURE', 'TURBINE_SPEED', 'GENERATOR_VOLTAGE', 'AIR_QUALITY'])\n",
        "\n",
        "# Input function for prediction (for ERROR_TYPE)\n",
        "predict_input_fn_error = tf.compat.v1.estimator.inputs.pandas_input_fn(\n",
        "    x=X_predict_error, shuffle=False\n",
        ")\n",
        "\n",
        "# Making predictions on the prediction dataset (for ERROR_TYPE)\n",
        "predicted_classes_error = list(classifier_error.predict(input_fn=predict_input_fn_error))\n",
        "\n",
        "# Extracting predicted classes\n",
        "predicted_classes_error = [prediction['class_ids'][0] for prediction in predicted_classes_error]\n",
        "\n",
        "# Printing the predicted classes for the prediction dataset (for ERROR_TYPE)\n",
        "print(\"The error type is : \")\n",
        "print(error_desc[predicted_classes_error[0]])"
      ],
      "metadata": {
        "id": "qWTKLJZkQr6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Error description\n",
        "error_desc = ['NO ERROR', 'ERROR A', 'ERROR B', 'ERROR C']\n",
        "\n",
        "# Defining features and target variable for training (for ERROR_TYPE)\n",
        "X_train_error = train_df[['FLU_GAS_TEMP', 'STEAM_PRESSURE', 'TURBINE_SPEED', 'GENERATOR_VOLTAGE', 'AIR_QUALITY']]\n",
        "y_train_error = train_df['ERROR_TYPE'].astype(int)  # Ensure labels are integers\n",
        "\n",
        "# Standardizing features for training (for ERROR_TYPE)\n",
        "scaler_error = StandardScaler()\n",
        "X_train_error = scaler_error.fit_transform(X_train_error)\n",
        "\n",
        "# Splitting the dataset into training and validation sets (for ERROR_TYPE)\n",
        "X_train_error, X_val_error, y_train_error, y_val_error = train_test_split(\n",
        "    X_train_error, y_train_error, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Building the DNN model for ERROR_TYPE using tf.keras\n",
        "model_error = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train_error.shape[1],)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(4, activation='softmax')  # Assuming 4 classes for ERROR_TYPE\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_error.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model_error.fit(X_train_error, y_train_error, epochs=10, batch_size=32, validation_data=(X_val_error, y_val_error))\n",
        "\n",
        "# Defining features for prediction (for ERROR_TYPE)\n",
        "X_predict_error = predict_df[['FLU_GAS_TEMP', 'STEAM_PRESSURE', 'TURBINE_SPEED', 'GENERATOR_VOLTAGE', 'AIR_QUALITY']]\n",
        "\n",
        "# Standardizing features for prediction (for ERROR_TYPE)\n",
        "X_predict_error = scaler_error.transform(X_predict_error)\n",
        "\n",
        "# Making predictions on the prediction dataset (for ERROR_TYPE)\n",
        "predicted_probabilities_error = model_error.predict(X_predict_error)\n",
        "\n",
        "# Extracting predicted classes\n",
        "predicted_classes_error = np.argmax(predicted_probabilities_error, axis=1)\n",
        "\n",
        "# Printing the predicted classes for the prediction dataset (for ERROR_TYPE)\n",
        "print(\"\\nThe error type for index \",i,\" of the evaluating dataset is: \",error_desc[predicted_classes_error[i]])"
      ],
      "metadata": {
        "id": "JsRshhH2cQwN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}